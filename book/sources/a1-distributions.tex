% !TEX root = ../script.tex
\section{Основные вероятностные распределения}

\subsection{Многомерное нормальное распределение}
\label{sec:gauss}
\index{распределение!многомерное нормальное}

Многомерное нормальное распределение или гауссовское распределение --- такое вероятностное распределение $p(\vecX | \vecMu, \Sigma)$ на $\vecX \in \bbR^\iD$, что его плотность имеет вид:
\[
p(\vecX | \vecMu, \Sigma) = \frac{1}{(2 \pi)^{\iD / 2} |\Sigma|^{1 / 2}}
\exp \left\{- \frac12 (\vecX - \vecMu)^{\mathrm{T}} \Sigma^{-1} (\vecX - \vecMu) \right\}.
\]
Два набора параметров распределения --- вектор $\vecMu$ и матрица $\Sigma$ --- определяют его среднее значение и ковариационную матрицу соответственно.
Такое распределение обозначают $\mathcal{N}(\vecMu, \Sigma)$.

У нормального распределения множество замечательных свойств, о которых можно прочитать в отдельных главах этой книги или в более общих книгах, таких как книга Бишопа~\cite{bishop2006pattern}.

\subsection{Распределение Дирихле}
\label{sec:dirichlet}
\index{распределение!Дирихле}

Носитель распределение Дирихле --- симплекс.
Для $k$-мерного распределения Дирихле симплекс есть множество точек, для которых:
\[
S_k = \left\{ \vecX: \sum_{i = 1}^k x_i = 1, x_i \geq 0, i \in \{1, \ldots, k\} \right\}.
\]
Легко видеть, что существует взаимнооднозначное соотвествие между вероятностными распределениями на конечном множестве $\{1, \ldots, k\}$ и точками такого симплекса.

Плотность распределения Дирихле с вектором параметров $\vecA \in \bbR_+^k (\alpha_i \geq 0)$ есть:
\[
p(\vecX | \vecA) \propto x_1^{\alpha_1 - 1} \cdot \ldots \cdot x_k^{\alpha_k - 1}.
\]

Получим нормировочный коэффициент для такого распределения:
\[
\int_{\vecX \in S_k} x_1^{\alpha_1 - 1} \cdot \ldots \cdot x_k^{\alpha_k - 1} d\vecX = \frac{\prod_{i = 1}^k \Gamma(\alpha_i)}{\Gamma(\sum_{i = 1}^k \alpha_k)},
\]
здесь $\Gamma(a) = \int_{0}^{\infty} t^{a - 1} e^{-t} dt$ --- гамма функция, $\Gamma(n + 1) = n!$ для $n \in \mathbb{N}$ и $\Gamma(a + 1) = a \Gamma(a)$.

Если мы рассмотрим $k = 2$, то получим бета-распределение.
В частности, выполнено, что:
\[
\int_0^1 \theta^{\alpha_1 - 1} (1 - \theta)^{\alpha_2 - 1} d\theta =  \frac{\Gamma(\alpha_1) \Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2)}.
\]

\begin{example}
Найдем среднее бета-распределения.
\begin{align*}
\bbE x &= \int_0^1 \theta \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \theta^{\alpha_1 - 1} (1 - \theta)^{\alpha_2 - 1}d\theta = \\
&= \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \int_0^1 \theta^{\alpha_1 + 1 - 1} (1 - \theta)^{\alpha_2 - 1}d\theta = \\
&= \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1) \Gamma(\alpha_2)} 
   \frac{\Gamma(\alpha_1 + 1) \Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2 + 1)} = \frac{\alpha_1}{\alpha_1 + \alpha_2}.
\end{align*}
Для распределения Дирихле с вектором параметров $\vecA$ математическое ожидание равно $\bbE x_j = \frac{\alpha_j}{\sum_{i = 1}^k \alpha_k}$.

\end{example}

\subsection{Экспоненциальное семейство распределений}
\label{sec:exp_family}
\index{экспоненциальное семейство распределений}
% http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf
% https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf

В этом разделе определим экспоненциальное семейство распределений и получим ряд его полезных свойств.
\begin{Definition}
Будем говорить, что распределение многомерной случайной величины $\vecX$ принадлежит экспоненциальному семейству, если
его плотность относительно меры Лебега $p(\vecX | \vecT)$ имеет вид:
\[
p(\vecX | \vecT) = h(\vecX) \exp \left(\vecT^\T T(\vecX) - A(\vecT) \right).
\]
\end{Definition}

Параметризация вектора параметров распределения $\vecT$, для которой правдоподобие имеет такой вид, называется \emph{канонической}. 

Вектор $T(\vecX)$ --- вектор достаточных статистик для модели, то есть такая функция данных $\vecX$, что
условное распределение $P(\vecX | \vecT)$ совпадает с условным распределением $P(\vecX | T, \vecT)$.
Эквивалентное утверждение, необходимое и достаточное условие того, что статистика является достаточной определяет взаимосвязь условных распределений $P(\vecT | \vecX, T) = P(\vecT | T)$.

Для экспоненциального семейства распределений вектор достаточных статистик конечен. Теорема Питмана-Купмана-Дармуа утверждает, что принадлежность к экспоненциальному семейству --- необходимое и достаточное условие для того, чтобы размер вектора достаточных статистик не зависел от размера выборки, если область возможных значений не зависит от вектора параметров.

Экспоненциальному семейству распределений принадлежат почти все используемые в математической статистике распределения: нормальное, биномиальное, Пуассоновское.
Среди известных распределений, которые не принадлежат этому семейству распределений, --- распределение Коши.

Приведем два примера экспоненциального семейства, канонических параметризаций и достаточных статистик для них.
\begin{example}
Рассмотрим распределение Бернулли, определенное на $x \in \{0, 1\}$.
\begin{align*}
p(x | \alpha) &= \alpha^{x} (1 - \alpha)^{1 - x} = \\
&= \exp \left[\log (\alpha^{x} (1 - \alpha)^{1 - x}) \right] = \\
&= \exp \left[x \log \alpha + (1 - x) \log (1 - \alpha)) \right] = \\
&= \exp \left[x \log \frac{\alpha}{1 - \alpha} + \log (1 - \alpha)) \right] = \\
&= \exp \left[x \theta - \log (1 + e^{\theta})\right].
\end{align*}
Для распределения Бернулли минимальная достаточная статистика
\[
T(x) = x, 
\]
Каноническая параметризация 
\[
\theta = \log \frac{\alpha}{1 - \alpha}, 
\]
а $A(\theta)$ и $h(\vecX)$:
\[
A(\theta) = \log (1 + e^{\theta}), h(\vecX) = 1.
\]
\end{example}

Покажем теперь, что нормальное распределение тоже принадлежит экспоненциальному семейству
\begin{example}
\begin{align*}
p(x| \mu, \sigma) &= \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2} (x - \mu)^2 \right) =\\
     &= \frac{1}{\sqrt{2 \pi}} \exp \left(-\log \sigma - \frac{x^2}{2 \sigma^2} + \frac{\mu x}{\sigma^2} - \frac{\mu^2}{2 \sigma^2} \right) = \\
    &= \frac{1}{\sqrt{2 \pi}} \exp \left( \vecT^\T T(x) - \log \sigma - \mu^2 / (2 \sigma^2) \right),
\end{align*}
$h(x) = \frac{1}{\sqrt{2 \pi}}$, $A(\vecT) = \log \sigma + \mu^2 / (2 \sigma^2)$.
Получаем вектор достаточных статистик:
\[
T(x) = \begin{pmatrix}
x \\
x^2 
\end{pmatrix},
\]
и каноническую параметризацию:
\[
\vecT = \begin{pmatrix}
\mu / \sigma^2 \\
-1 / (2 \sigma^2).
\end{pmatrix}
\]
Компонента $A(\theta)$ имеет следующий вид как функция от канонических параметров:
\[
A(\theta) = \frac{\mu}{2 \sigma^2} + \log \sigma = - \frac{\theta_1^2}{4 \theta_2} - \frac12 \log (-2\theta_2).
\]
\end{example}

Приведем еще три важных свойства распределений из этого семейства.
\begin{Theorem}
\label{th:exponential_derivative}
\begin{equation}
\label{eq:a_derivative}
\frac{\partial A(\vecT)}{\partial \vecT} = \bbE_{p_{\vecT}} T(\vecX).
\end{equation}
\end{Theorem}
\begin{proof}
Интеграл плотности вероятностного распределения $p(\vecX| \vecT)$ равен $1$:
\[
1 = \int_{\bbR^{\iD}} h(\vecX) \exp \left(\vecT^T T(\vecX) - A(\vecT) \right) d\vecX.
\]
Перенесем $\exp \left(A(\vecX) \right)$ и прологарифмируем левую и правую часть:
\[
A(\vecT) = \log \left[\int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX)) d \vecX \right].
\]
Обозначим $Q(\vecT) = \int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX)) d \vecX$.
Подсчитаем производную:
\begin{align*}
\frac{\partial A(\vecT)}{\partial \vecT} &= \frac{1}{Q(\vecT)} \frac{\partial Q(\vecT)}{\partial \vecT} = \frac{Q'(\vecT)}{Q(\vecT)} = \\
&= \frac{\int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX)) T(\vecX) d \vecX}{\int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX)) d \vecX} = \\
&= \frac{\int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX) - A(\vecT)) T(\vecX) d \vecX}{\int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX) - A(\vecT)) d \vecX} = \\
&= \int_{\bbR^{\iD}} h(\vecX) \exp( \vecT^\T T(\vecX) - A(\vecT)) T(\vecX) d \vecX = \\
&= \bbE_{p_{\vecT}} T(\vecX).
\end{align*}
Получается, что мы явно можем выразить эту производную как математическое ожидание достаточной статистики:
\[
\frac{\partial A(\vecT)}{\partial \vecT} = \bbE_{p_{\vecT}} T(\vecX).
\]
\end{proof}
\begin{Theorem}
Функция $A(\vecT)$ выпуклая.
\end{Theorem}
\begin{proof}
Возьмем вторую производную от $\frac{\partial A(\vecT)}{\partial \vecT}$, то получим:
\begin{align*}
\frac{\partial A'(\vecT)}{\partial \vecT} &= \frac{\partial }{\partial \vecT} \left[\frac{Q'(\vecT)}{Q(\vecT)} \right] = \frac{\partial }{\partial \vecT} \left[ Q'(\vecT) \frac{1}{Q(\vecT)} \right] = \\
&=  \frac{\int h(\vecX) \exp \left(\vecT^T T(\vecX)  - A(\vecT)\right) T^2(\vecX) d \vecX}{\int h(\vecX) \exp \left(\vecT^T T(\vecX)  - A(\vecT)\right) d \vecX} - \left(\bbE_{p_{\vecT}} T(\vecX) \right)^2 = \\
&= \bbE_{p_{\vecT}} T^2(\vecX) - \left(\bbE_{p_{\vecT}} T(\vecX) \right)^2 = \mathrm{cov}_{p_{\vecT}} T(\vecX).
\end{align*}
То есть, гессиан $A(\vecT)$ --- ковариационная матрица:
% TODO вставить вывод
\[
\frac{\partial^2 A(\vecT)}{\partial \vecT^2} = \mathrm{cov}_{p_{\vecT}} T(\vecX).
\]
Ковариационная матрица случайного вектора $\mathrm{cov}_{p_{\vecT}} T(\vecX)$ неотрицательно определена.
Следовательно, функция $A(\vecT)$ выпуклая.
\end{proof}

Наконец обозначим $\vecMu = \bbE T(\vecX)$.
\begin{Theorem}
Пусть мы наблюдаем выборку независимых одинаково распределенных случайных величин $D = \{x_1, \ldots, x_n\}$.
Тогда оценка максимума правдоподобия $\hat{\vecMu}_{MLE}$:
\[
\hat{\vecMu}_{MLE} = \frac{1}{n} \sum_{i = 1}^{n} T(x_i).
\]
\end{Theorem}
\begin{proof}
Используем Теорему~\ref{th:exponential_derivative} и явно дифференцируем плотность.
\end{proof}

Так как выполнено~\eqref{eq:a_derivative}, 
то решение уравнения 
\[
\frac{d A(\vecT)}{d \vecT} = \frac{1}{n} \sum_{i = 1}^{n} T(x_i).
\]
может использоваться для оценки вектора параметров $\vecT$

Оценки максимума правдоподобия $\hat{\vecMu}_{MLE}$ будут несмещенными и эффективными (для них будет выполнено неравенство Рао-Крамера).

% TODO написать про это