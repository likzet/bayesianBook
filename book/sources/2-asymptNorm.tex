% !TEX root = ../script.tex
\section{Асимптотическая нормальность апостериорного распределения}

В классической статистике важным является установить для оценки ее асимптотическое поведение.
Большинство используемых оценок --- регулярные, для них
можно установить асимптотическую нормальность.

Для Байесовских оценок условие сходимости апостериорного распределения к нормальному определяют теорема Бернштейна-фон Мизеса и в более общем смысле условия Ибрагимов и Хасьминского.
Оба результата представлены в этом разделе.


\subsection{Теорема Бернштейна-фон Мизеса}

Важной проблемой Байесовской статистики с точки зрения обычной математической статистики является несоответствие между Байесовскими оценками и эффективными классическими оценка.

Оказывается, что в асимптотике Байесовские оценки часто совпадают с классическими.
Формальное утверждение про близость Байесовских и классических оценок 
составляет теорема суть теоремы Бернштейна-фон Мизеса.

Рассмотрим выборку независимых одинаково распределенных величин $X = \{\vecX_1, \ldots, \vecX_{\sS} \}$ из распределения с плотностью $f(\vecX | \theta)$. 
Параметр $\theta \in \Theta$, $\Theta$ --- открытое подмножество $\bbR$.
Будем предполагать следующие условия регулярности:
\begin{itemize}
	\item[A1] Носитель $f(\vecX | \theta)$ одинаков для всех $\theta \in \Theta$
	\item[A2] Логарифм правдоподобия $l(\vecX | \theta) = \log p(\vecX | \theta)$ трижды непрерывно дифференцируем по $\theta$ в окрестности истинного значения $(\theta_0 - \delta, \theta_0 + \delta)$. Обозначим $\dot{l}(\vecX | \theta)$,
	$\ddot{l}(\vecX | \theta)$, $\dddot{l}(\vecX | \theta)$ первую, вторую и третью частные производные по параметру правдоподобия. Пусть математические ождидания $\bbE_{\theta_0} \dot{l}(\vecX | \theta)$, 
	$\bbE_{\theta_0} \ddot{l}(\vecX | \theta)$ конечны, и
	\[
	\sup_{\theta \in (\theta_0 - \delta, \theta_0 + \delta)} |\dddot{l}(\vecX | \theta)| < M(\vecX),
	\]
	причем $\bbE_{\theta_0} M(\vecX) < \infty$.
	\item[A3] Можно менять местами математическое ожидание по $\theta_0$и дифференцирование по $\theta_0$, так что
	\begin{align*}
	\bbE_{\theta_0} \dot{l}(\vecX | \theta_0) &= 0 \\
	\bbE_{\theta_0} \ddot{l}(\vecX | \theta_0) &= - \bbE_{\theta_0} (\dot{l}(\vecX | \theta))^2.
	\end{align*}
	\item[A4] Информация Фишера\index{информация Фишера} $I(\theta_0)^2 = \bbE_{\theta_0} (\dot{l}(\vecX | \theta_0))^2 > 0$.
\end{itemize}

В таких предположениях состоятельная оценка максимума правдоподобия
будет асимптотически нормальной.

\begin{Theorem}
Пусть для семейства плотностей $\{f(\vecX|\theta), \theta \in \Theta\}$ выполнены предположения [A1]-[A4] и оценка максимума правдоподобия $\mleT_{\sS}$ состоятельна, то
\[
\sqrt{\sS} (\mleT_{\sS} - \theta_0) \rightarrow^D \mathcal{N} \left(0, \frac{1}{I(\theta_0)}\right).
\]
\end{Theorem}

\begin{proof}
Утверждение теоремы следует из центральной предельной теоремы и усиленного закона больших чисел. 

Обозначим $l_\sS(\theta) = \sum_{i = 1}^{\sS} l(\vecX_i | \theta)$,
а ее первую, вторую и третью производную по $\theta$ --- $\dot{l}_\sS(\theta), \ddot{l}_\sS(\theta)$ и $\dddot{l}_\sS(\theta)$ соответственно.
Разложим производную $l_\sS(\theta)$ по Тейлору:
\[
0 = \dot{l}_{\sS}(\mleT_\sS) = \dot{l}_{\sS}(\theta_0) + (\mleT_\sS - \theta_0) \ddot{l}_{\sS}(\theta_0) + \frac12 (\mleT_\sS - \theta_0)^2 \dddot{l}_{\sS}(\theta'),
\]
где $\theta_0 \leq \theta' \leq \mleT_\sS$.
Тогда
\[
\sqrt{\sS} (\mleT_\sS - \theta_0) = \frac{\frac{1}{\sqrt{\sS}} \dot{l}_{\sS}(\theta_0)}{-\frac{1}{\sS} \ddot{l}_{\sS}(\theta_0) - \frac12 \frac{1}{\sS} \dddot{l}_{\sS}(\theta')}. 
\]
Так как выполнена центральная предельная теорема, то числитель сходится по распределению к $\mathcal{N}(0, I(\theta_0))$.
Первое слагаемое в знаменателе сходится к $I(\theta_0)$ по усиленному закону больших чисел.
Второе слагаемое мало в силу состоятельности $\mleT_\sS$ и ограниченности $|\dddot{l}(\vecX | \theta)|$.
Следовательно, левая часть равенства сходится по распределению к 
$\mathcal{N}(0, 1 / I(\theta_0))$.
% https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf
% - похожее доказательство, но с разложением только до первого порядка
\end{proof}

Для теоремы Бернштейна-фон Мизеса понадобятся дополнительные предположения:
\begin{itemize}
	\item[A5] Для произвольного $\delta > 0$ существует $\varepsilon > 0$такое, что
	\[
	P_{\theta_0} \left\{ \sup_{|\theta - \theta_0| > \delta} \frac{1}{\sS} \left( l_\sS(\theta) - l_\sS(\theta_0) \right) \leq -\varepsilon \right\} \rightarrow 1.
	\]
	\item[A6] Априорная плотность распределения $\pi(\theta)$ непрерывна и положительна $\theta_0$.
\end{itemize}

\begin{Theorem}[Теорема Бернштейна-фон Мизеса]
\index{теорема Бершнейтна-фон Мизеса}
Пусть выполнены предположения [A1]-[A6], и $\mleT_\sS$ --- состоятельная оценка максимума правдоподобия.
Обозначим совместную плотность выборки $f(X| \theta)$.
Тогда для $\sS \rightarrow \infty$:
\[
\int_\bbR \left|p(s | X) - \frac{1}{\sqrt{2\pi}\sqrt{I(\theta_0)^{-1}}} \exp \left(-\frac{1}{2 I(\theta_0)^{-1}} s^2 \right) \right| ds \rightarrow^{p} 0,
\]
где $s = \sqrt{\sS} (\theta - \mleT_\sS(X))$.
\end{Theorem}

Доказательство этого результата --- техническое: нужно разбить область интегрирования на три и в каждой из областей оценить интеграл сверху.

Эта теорема утверждает, что для выбранного априорного распределения мы можем точно описать апостериорное распределение.

Как следствие этой теоремы мы получаем асимптотическую нормальность и сходимость для Байесовской оценки.
\begin{Theorem}
Пусть $\int_\Theta |\theta| \pi(\theta) d\theta < \infty$. 
Будем использовать в качестве Байесовской оценки апостериорное среднее:
\[
\theta^*_\sS = \int_{\Theta} \theta p(\theta | X) d\theta.
\]
Тогда
\[
\sqrt{\sS} (\mleT_\sS - \theta^*_\sS) \rightarrow^{p_{\theta_0}} 0.
\]
Кроме того, 
\[
\sqrt{\sS} (\theta^*_\sS - \mleT_\sS) \rightarrow^{D} \mathcal{N}\left(0, \frac{1}{I(\theta_0)}\right).
\]
\end{Theorem}

Существуют вариации представленных результатов, полученные в других предположениях о регулярности семейства.
Например, получена версия теоремы Бернштейна-фон Мизеса при нарушении параметрического предположения и для конечных выборок.
% были, например, получены в достаточно общем случае В.Спокойным, 
% spokoiny2012parametric
%а для конкретных моделей --- вашим покорным слугой, Е.Бурнаевым и М.Пановым.

\subsection{Условия Ибрагимова и Хасьминского}

Ибрагимов и Хасьминский предложили ряд условий для целого семейства параметрических моделей.
Эти условия были проверены для различных классов нерегулярных задач и случайных процессов.
Рассмотрим теперь результаты, которые получаются с использованием этих условий.

Множество значений параметров $\Theta$ является подмножеством пространства $\bbR^{\pD}$.
Для упрощения изложения рассмотрим $\pD = 1$.
Совместное распределение выборки $\Sample = \{\vecX_1, \ldots, \vecX_{\sS}\}$ обозначим $P_{\theta}^{\sS}$, а плотность относительно сигма-конечной меры обозначим $p(\Sample, \theta)$.
Последовательность положительных констант $\phi_{\sS}$ сходится к $0$ при $\sS \rightarrow \infty$.
% случай k>1
В регулярном случае, рассмотренном в предыдущем разделе, можно взять $\phi_{\sS} = \frac{1}{\sqrt{\sS}}$.
В нерегулярном случае, как правило, сходимость $\phi_{\sS} \rightarrow 0$ может быть быстрее.
Рассмотрим отображение $U$, определенное как $U(\theta) = \frac{1}{\phi_{\sS}} (\theta - \theta_0)$, где $\theta_0$ --- истинное значение параметра.
Пусть $\mathcal{U}_{\sS} = \{U(\theta): \theta \in \Theta \}$.
Величина $u$ является соответствующим образом масштабированной разностью между $\theta$ и $\theta_0$.
Зададим случайный процесс
\[
Z_{\sS}(u, \Sample_{\sS}) = \frac{p(\Sample_{\sS}, \theta_0 + \phi_{\sS} u)}{p(\Sample_{\sS}, \theta_0)}.
\]

\textit{Условия Ибрагимова-Хасьминского} имеют вид:
\begin{itemize}
\item[ИХ1] Для некоторых $M > 0$, $m_1 \geq 0, \alpha > 0, \sS_0 \geq 1$ выполнено, что
\begin{align*}
\E_{\theta_0} \| Z_{\sS}^{\frac12}(u_1) - Z_{\sS}^{\frac12}(u_2)\|^2 &\leq M (1 + A^{m_1}) |u_1 - u_2|^{\alpha}, \\
\forall u_1, u_2 \in \mathcal{U}_{\sS} \text{ with } &|u_1| \leq A, |u_2| \leq A
\end{align*}
для всех $\sS \geq \sS_0$.
% TODO про расстояние Кульбака-Лейблера
\item[ИХ2] Для всех $u \in \mathcal{U}_{\sS}$ и $\sS \geq \sS_0$ 
\[
\E_{\theta_0} \|Z_{\sS}^{\frac12} (u) \| \leq \exp (-g_{\sS}(|u|)),
\]
где $g_{\sS}$ --- последовательность действительнозначных функций, удовлетворяющих следующим условиям:
\begin{itemize}
\item для любого $\sS \geq 1$, $g_{\sS}(y) \uparrow \infty$ для $y \rightarrow \infty$,
\item[ИХ3] для любого $N > 0$
\[
\lim_{y \rightarrow \infty, \\ \sS \rightarrow \infty} y^N \exp(-g_{\sS}(y)) = 0.
\]
\end{itemize}
\item Конечномерные распределения $\{Z_{\sS}(u): u \in \mathcal{U}_{\sS}\}$ сходятся к конечномерным распределениям случайного процесса $\{Z(u): u \in \mathbb{R}\}$.
\end{itemize}

% TODO про iid выборку

\begin{Theorem}
Пусть $\Pi$ --- априорное распределение с положительной непрерывной плотностью в $\theta_0$. 
Тогда если выполнены условия Ибрагимова-Хасьминского [ИХ1--ИХ3] для квадратичной функции потерь, нормализованная Байесовская оценка $\phi_{\sS} (\tilde{\theta}_{\sS} - \theta_0)$ сходится по распределению к $\int u Z(u) du / \int Z(u) du$.
\end{Theorem}

\begin{Proposition}
Предположим, что $\vecX_1, \ldots, \vecX_{\sS}$ --- независимые одинаково распределенные случайные величины, и $\Pi$ --- априорное распределение.
Пусть $\hat{\theta}(\vecX_1, \ldots, \vecX_{\sS})$ --- симметричная функция по $\vecX_1, \ldots, \vecX_{\sS}$. Обозначим
\[
t = \phi_{\sS}^{-1} (\theta - \hat{\theta}(\Sample_{\sS})),
\]
и $A$ --- борелевское множество.
Пусть
\[
\Pi(t \in A| \Sample_{\sS}) \rightarrow^{P_{\theta_0}} Y_A.
\]
Тогда $Y_A$ --- константа почти всюду на $P_{\theta_0}$.
\end{Proposition}

\begin{Definition}
Для некоторой симметрической функции $\hat{\theta}(\Sample_{\sS})$ апостериорное распределение $t = \phi_{\sS}^{-1} \left(\theta - \hat{\theta}(\Sample_{\sS}) \right)$ сходится к $Q$, если 
\[
\sup_{A} \{\Pi(t \in A| \Sample_{\sS}) - Q(A)\} \rightarrow^{P_{\theta_0}} 0.
\]
Тогда $\hat{\theta}(\Sample_{\sS})$ называют \emph{точным центрированием.}
\end{Definition}

\begin{Theorem}
Пусть выполнены условия Ибрагимова-Хасьминского и $\Pi$ --- априорное распределение с непрерывной положительной плотностью в $\theta_0$. 
Если точное центрирование $\hat{\theta}(\Sample_{\sS})$ существует, тогда существует случайная величина $W\!$, такая, что
\begin{itemize}
\item[a)] $\phi_{\sS}^{-1}(\theta_0 - \hat{\theta}(\vecX_1, \ldots, \vecX_{\sS}))$ сходится по распределению к $W$.
\item[b)] Для почти всех $\eta \in \mathbb{R}$ величина $\xi(\eta - W) = q(\eta)$ является неслучайной. Здесь $\xi(u) = Z(u) / \int_{\mathbb{R}} Z(u) du, u \in \mathbb{R}$.

Если b) выполнено для некоторой случайной величины $W$, то апостериорное среднее для заданной выборки $\Sample_\sS$ является точным центрированием с $Q(A) = \int_{A} q(t) dt$.
\end{itemize}
\end{Theorem}
